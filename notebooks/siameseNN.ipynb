{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torch.optim import Adam \n",
    "\n",
    "from statistics import mean\n",
    "from random import choices, choice, seed\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese Neural Network for object classification\n",
    "We use a resnet as the backbone of our Siamese Neural Network (SNN) with a simple feed \n",
    "forward neural net on top as a classifier. Our SNN takes two images and outputs their \n",
    "similarity where numbers close to 0 indicate similar images and numbers close to 1 indicate \n",
    "dissimilar images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNN(nn.Module): \n",
    "    def __init__(self, backbone=None, distance_dim=4096): \n",
    "        super(SiameseNN, self).__init__()\n",
    "        self.distance_dim = distance_dim\n",
    "        if backbone is None: \n",
    "            self.backbone = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        else: \n",
    "            self.backbone = backbone\n",
    "        self.sim = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(1000, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, base_im, cmp_im): \n",
    "        out_1 = self.backbone(base_im)\n",
    "        out_2 = self.backbone(cmp_im)\n",
    "        return self.sim(torch.abs(out_1 - out_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet Dataset and Loss\n",
    "We will be training our Siamese Neural Net on the CIFAR100 dataset. Each training sample \n",
    "consists of three images. Two images will be of the same class called the anchor and \n",
    "positive image, and the last will be from a different class called the negative image. \n",
    "Denote the anchor image by $a$, the positive image by $p$, and the negative image by $n$. \n",
    "We also set a hyperparameter $m$ as the margin\n",
    "Our loss is given by: \n",
    "\n",
    "$$L(a, n, p) = \\max(SNN(a, p) - SNN(a, n) + m, 0)$$\n",
    "\n",
    "## Observations about the loss\n",
    "- The loss is minimized when the SNN predicts the anchor and positive image as similar \n",
    "- The loss is minimized when the SNN predicts the anchor and negative image as dissimilar\n",
    "- The loss is 0 when the difference in similarity is greater than the margin\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triplet_Loss: \n",
    "    def __init__(self, margin) -> None:\n",
    "        self.margin = margin\n",
    "\n",
    "    def __call__(self, dist_same, dist_diff):\n",
    "        return torch.mean(torch.clamp(dist_same - dist_diff + self.margin,min=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation\n",
    "The CIFAR100 dataset has 100 different classes. We will take a subset of these classes\n",
    "and use them to train the model. We will test how well the model generalizes by using the \n",
    "rest of the classes to validate. Ideally we will see that the model can tell that images \n",
    "of the same class are similar, on classes that weren't seen in the training set. \n",
    "\n",
    "## Specifics of the Dataset\n",
    "We create a sample in the training set by the following procedure: \n",
    "1. split the 100 different classes for training or validation \n",
    "2. sample a class from the list of training classes\n",
    "3. sample a class from the list of training classes which is not the class sampled in (2)\n",
    "4. randomly sample two images from class chosen in (2). These are the anchor and positive image. \n",
    "5. randomly sample an image from the class chosen in (3). This is the negative image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triplet_Dataset(torch.utils.data.Dataset): \n",
    "    def __init__(self, train_classes, num_samples, seed, train=True) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "        self.fullset = torchvision.datasets.CIFAR100(root='../data/processed', train=True,\n",
    "                                                    download=True, transform=transform)\n",
    "        \n",
    "        self.train_classes = train_classes\n",
    "        self.num_samples = num_samples\n",
    "        self.seed = seed\n",
    "        \n",
    "        self._create_dataset(train=train)\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.anchors)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        return self.anchors[idx, ...], self.im_positive[idx, ...], self.im_negative[idx, ...]\n",
    "    \n",
    "    def create_label_map(self): \n",
    "        seed(self.seed)\n",
    "        labels = {}\n",
    "        for i in range(len(self.fullset)): \n",
    "            if self.fullset[i][1] in labels: \n",
    "                labels[self.fullset[i][1]].append(i)\n",
    "            else: \n",
    "                labels[self.fullset[i][1]] = [i]\n",
    "        return labels\n",
    "\n",
    "    def train_test_split(self): \n",
    "        train_classes = choices(list(range(100)), k=self.train_classes)\n",
    "        valid_classes = [i for i in range(100) if i not in train_classes]\n",
    "        return train_classes, valid_classes\n",
    "\n",
    "    def _create_dataset(self, train=True): \n",
    "        anchors = torch.zeros((self.num_samples, 3, 32, 32), dtype=torch.float)\n",
    "        im_positive = torch.zeros((self.num_samples, 3, 32, 32), dtype=torch.float)\n",
    "        im_negative = torch.zeros((self.num_samples, 3, 32, 32), dtype=torch.float)\n",
    "        train_classes, valid_classes = self.train_test_split()\n",
    "        label_map = self.create_label_map()\n",
    "        potential_classes = train_classes\n",
    "        if not train: \n",
    "            potential_classes = valid_classes\n",
    "        for idx in range(self.num_samples): \n",
    "            true_class = choice(potential_classes)\n",
    "            false_class = choice(potential_classes)\n",
    "            while false_class == true_class: \n",
    "                false_class = choice(potential_classes)\n",
    "            anchor_idx = choice(label_map[true_class])\n",
    "            pos_idx = choice(label_map[true_class])\n",
    "            neg_idx = choice(label_map[false_class])\n",
    "            anchors[idx, :, :, :] = self.fullset[anchor_idx][0]\n",
    "            im_positive[idx, :, :, :] = self.fullset[pos_idx][0]\n",
    "            im_negative[idx, :, :, :] = self.fullset[neg_idx][0]\n",
    "        self.anchors, self.im_positive, self.im_negative = anchors, im_positive, im_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(loader, model, loss):\n",
    "    same_score = []\n",
    "    diff_score = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    with torch.no_grad(): \n",
    "        for anchor, pos, neg in loader: \n",
    "            anchor, pos, neg = anchor.to(device), pos.to(device), neg.to(device)\n",
    "            dist_pos = model(anchor, pos)\n",
    "            dist_neg = model(anchor, neg)\n",
    "            \n",
    "            same = dist_pos.reshape(-1).tolist()\n",
    "            same_score.extend(same)\n",
    "\n",
    "            diff = dist_neg.reshape(-1).tolist()\n",
    "            diff_score.extend(diff)\n",
    "    return mean(same_score), mean(diff_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'lr': .00001, \n",
    "        'batchsize': 32, \n",
    "        'betas': (.9, .99), \n",
    "        'distance_dim': 4096, \n",
    "        'epochs': 100,\n",
    "        'margin': .9,\n",
    "        'train_classes': 75, \n",
    "        'train_samples': 50_000, \n",
    "        'val_samples': 10_000\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using gpu: True\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "epoch: 0, train_error: 0.9005, train_dist_pos: 0.463, train_dist_neg: 0.462, val_dist_pos: 0.46, val_dist_neg: 0.461\n",
      "epoch: 1, train_error: 0.8975, train_dist_pos: 0.466, train_dist_neg: 0.468, val_dist_pos: 0.494, val_dist_neg: 0.481\n",
      "epoch: 2, train_error: 0.8557, train_dist_pos: 0.456, train_dist_neg: 0.5, val_dist_pos: 0.475, val_dist_neg: 0.455\n",
      "epoch: 3, train_error: 0.7789, train_dist_pos: 0.432, train_dist_neg: 0.553, val_dist_pos: 0.557, val_dist_neg: 0.525\n",
      "epoch: 4, train_error: 0.7057, train_dist_pos: 0.375, train_dist_neg: 0.569, val_dist_pos: 0.365, val_dist_neg: 0.334\n",
      "epoch: 5, train_error: 0.6291, train_dist_pos: 0.319, train_dist_neg: 0.591, val_dist_pos: 0.415, val_dist_neg: 0.389\n",
      "epoch: 6, train_error: 0.5844, train_dist_pos: 0.285, train_dist_neg: 0.605, val_dist_pos: 0.263, val_dist_neg: 0.244\n",
      "epoch: 7, train_error: 0.5676, train_dist_pos: 0.28, train_dist_neg: 0.62, val_dist_pos: 0.26, val_dist_neg: 0.223\n",
      "epoch: 8, train_error: 0.5529, train_dist_pos: 0.275, train_dist_neg: 0.634, val_dist_pos: 0.289, val_dist_neg: 0.272\n",
      "epoch: 9, train_error: 0.5225, train_dist_pos: 0.245, train_dist_neg: 0.638, val_dist_pos: 0.382, val_dist_neg: 0.359\n",
      "epoch: 10, train_error: 0.51, train_dist_pos: 0.244, train_dist_neg: 0.652, val_dist_pos: 0.171, val_dist_neg: 0.173\n",
      "epoch: 11, train_error: 0.5117, train_dist_pos: 0.253, train_dist_neg: 0.662, val_dist_pos: 0.13, val_dist_neg: 0.134\n",
      "epoch: 12, train_error: 0.5185, train_dist_pos: 0.285, train_dist_neg: 0.688, val_dist_pos: 0.114, val_dist_neg: 0.137\n",
      "epoch: 13, train_error: 0.522, train_dist_pos: 0.282, train_dist_neg: 0.684, val_dist_pos: 0.617, val_dist_neg: 0.575\n",
      "epoch: 14, train_error: 0.4981, train_dist_pos: 0.263, train_dist_neg: 0.691, val_dist_pos: 0.082, val_dist_neg: 0.09\n",
      "epoch: 15, train_error: 0.4919, train_dist_pos: 0.291, train_dist_neg: 0.725, val_dist_pos: 0.074, val_dist_neg: 0.08\n",
      "epoch: 16, train_error: 0.5328, train_dist_pos: 0.294, train_dist_neg: 0.688, val_dist_pos: 0.082, val_dist_neg: 0.088\n",
      "epoch: 17, train_error: 0.5143, train_dist_pos: 0.294, train_dist_neg: 0.709, val_dist_pos: 0.17, val_dist_neg: 0.17\n",
      "epoch: 18, train_error: 0.605, train_dist_pos: 0.39, train_dist_neg: 0.709, val_dist_pos: 0.106, val_dist_neg: 0.126\n",
      "epoch: 19, train_error: 0.5351, train_dist_pos: 0.33, train_dist_neg: 0.723, val_dist_pos: 0.116, val_dist_neg: 0.141\n",
      "epoch: 20, train_error: 0.5121, train_dist_pos: 0.305, train_dist_neg: 0.724, val_dist_pos: 0.247, val_dist_neg: 0.266\n",
      "epoch: 21, train_error: 0.5489, train_dist_pos: 0.332, train_dist_neg: 0.713, val_dist_pos: 0.089, val_dist_neg: 0.088\n",
      "epoch: 22, train_error: 0.4868, train_dist_pos: 0.282, train_dist_neg: 0.73, val_dist_pos: 0.116, val_dist_neg: 0.122\n",
      "epoch: 23, train_error: 0.4891, train_dist_pos: 0.313, train_dist_neg: 0.757, val_dist_pos: 0.093, val_dist_neg: 0.122\n",
      "epoch: 24, train_error: 0.5324, train_dist_pos: 0.294, train_dist_neg: 0.694, val_dist_pos: 0.124, val_dist_neg: 0.129\n",
      "epoch: 25, train_error: 0.5381, train_dist_pos: 0.339, train_dist_neg: 0.732, val_dist_pos: 0.484, val_dist_neg: 0.484\n",
      "epoch: 26, train_error: 0.5082, train_dist_pos: 0.277, train_dist_neg: 0.704, val_dist_pos: 0.087, val_dist_neg: 0.098\n",
      "epoch: 27, train_error: 0.4791, train_dist_pos: 0.243, train_dist_neg: 0.701, val_dist_pos: 0.107, val_dist_neg: 0.133\n",
      "epoch: 28, train_error: 0.4816, train_dist_pos: 0.262, train_dist_neg: 0.717, val_dist_pos: 0.109, val_dist_neg: 0.116\n",
      "epoch: 29, train_error: 0.4818, train_dist_pos: 0.262, train_dist_neg: 0.715, val_dist_pos: 0.101, val_dist_neg: 0.103\n",
      "epoch: 30, train_error: 0.4712, train_dist_pos: 0.263, train_dist_neg: 0.727, val_dist_pos: 0.552, val_dist_neg: 0.531\n",
      "epoch: 31, train_error: 0.485, train_dist_pos: 0.289, train_dist_neg: 0.737, val_dist_pos: 0.078, val_dist_neg: 0.075\n",
      "epoch: 32, train_error: 0.4882, train_dist_pos: 0.325, train_dist_neg: 0.771, val_dist_pos: 0.577, val_dist_neg: 0.554\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m        optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m        optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 41\u001b[0m        errs\u001b[38;5;241m.\u001b[39mappend(\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     42\u001b[0m    val_same_mean, val_diff_mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(val_loader, model, loss)\n\u001b[1;32m     43\u001b[0m    \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mepoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, train_error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(mean(errs), \u001b[38;5;241m4\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, train_dist_pos: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(mean(same_score), \u001b[38;5;241m3\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124mtrain_dist_neg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(mean(diff_score), \u001b[38;5;241m3\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val_dist_pos: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(val_same_mean, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124mval_dist_neg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(val_diff_mean, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'using gpu: {torch.cuda.is_available()}')\n",
    "\n",
    "model = SiameseNN(distance_dim=config['distance_dim']).to(device)\n",
    "optimizer = Adam(params=model.parameters(), lr=config['lr'], betas=config['betas'])\n",
    "    \n",
    "    \n",
    "trainset = Triplet_Dataset(config['train_classes'], config['train_samples'], 0, train=True)\n",
    "valset = Triplet_Dataset(config['train_classes'], config['val_samples'], 0, train=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=config['batchsize'],\n",
    "                                        shuffle=True, num_workers=8)\n",
    "val_loader = torch.utils.data.DataLoader(valset, batch_size=config['batchsize'],\n",
    "                                        shuffle=True, num_workers=8)\n",
    "\n",
    "loss = Triplet_Loss(margin=config['margin'])\n",
    "\n",
    "model.train()\n",
    "for e in range(config['epochs']): \n",
    "    errs = []\n",
    "    same_score = []\n",
    "    diff_score = []\n",
    "    acc = 0\n",
    "    model.train()\n",
    "    for anchor, pos, neg  in train_loader: \n",
    "        anchor, pos, neg = anchor.to(device), pos.to(device), neg.to(device)\n",
    "        dist_pos = model(anchor, pos)\n",
    "        dist_neg = model(anchor, neg)\n",
    "        \n",
    "        err = loss(dist_pos, dist_neg)\n",
    "\n",
    "        same = dist_pos.reshape(-1).tolist()\n",
    "        same_score.extend(same)\n",
    "\n",
    "        diff = dist_neg.reshape(-1).tolist()\n",
    "        diff_score.extend(diff)\n",
    "\n",
    "        err.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        errs.append(err.item())\n",
    "    val_same_mean, val_diff_mean = eval(val_loader, model, loss)\n",
    "    print(f\"\"\"epoch: {e}, train_error: {round(mean(errs), 4)}, train_dist_pos: {round(mean(same_score), 3)},\\\n",
    " train_dist_neg: {round(mean(diff_score), 3)}, val_dist_pos: {round(val_same_mean, 3)},\\\n",
    " val_dist_neg: {round(val_diff_mean, 3)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issues\n",
    "The main issure is generalization. The training loss is decreasing, but the validation loss is not. We hope that the metric between two images generalizes to classes we have never seen before but this is certainly not the case. \n",
    "\n",
    "## Observations\n",
    "- validation loss fluctuates wildly (maybe weights are changing too rapidly)\n",
    "\n",
    "## Things I have tried\n",
    "- playing around with the learning rate. \n",
    "- Tried different loss functions (binary cross entropy and contrastive loss)\n",
    "- Increased data (CIFAR10 to CIFAR100 and increased number of samples to 50_000)\n",
    "- various batchsizes (64, 32, 16)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('ota')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e4541c7a382996c1783d26d80198fcb6d98d6b25f50fc5af4b187d51d35d604"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
